### package.json and package.lock.json files?

In a Node.js project, the package.json file is mandatory, while the package-lock.json file is not mandatory but highly recommended for projects using npm for package management.

package.json is a manifest file that contains metadata about the project and specifies its dependencies. It defines the project configuration. It includes details such as the project name, version, entry point, script commands, and most importantly, the list of dependencies and their versions.

package.lock.json is automatically generated by npm, when dependencies are installed or updated. It contains a detailed record of the exact versions of all dependencies that were installed for the project at a specific point in time. this file is crucial for enabling reproducible and consistent builds in a project.

---

### Difference between the Asynchronous and non-blocking code?

Asynchronous code refers to the style of programming where operations are executed independently of the main program flow. Instead of waiting for each operation to complete before moving on to the next one, asynchronous code initiates an operation and then proceeds to execute subsequent code without waiting for the operation to finish.
Asynchronous code typically involves the use of callbacks, promises, async/await, and event-driven mechanisms. It allows the program to achieve concurrency, enabling multiple operations to be executed simultaneously.
Non-blocking code refers to the ability of a program to continue executing without being obstructed by long-running operations. In a non-blocking system, when an operation is initiated, the program can continue executing other tasks without waiting for the operation to be completed.
Non-blocking behavior is often associated with I/O operations and event-driven architectures.

---

### Difference between pipe and chaining in a node?

Pipe: In Node.js, the “pipe” method is used to direct the output of one stream to the input of another stream. It is commonly used with readable and writable streams to efficiently transfer data from one source to another without explicitly having to manage the data flow

```
const fs = require('fs');

const readableStream = fs.createReadStream('input.txt');
const writableStream = fs.createWriteStream('output.txt');

readableStream.pipe(writableStream);
```

Chaining in Node.js generally refers to method chaining, which is the practice of calling multiple methods on an object in a single statement, with each method returning the object itself, thereby allowing for a sequence of operations to be performed on the object in a linear fashion.

```
const result = [1, 2, 3, 4, 5]
  .map(num => num * 2)
  .filter(num => num > 5)
  .reduce((acc, num) => acc + num, 0);

console.log(result); // Output: 24
```
Both concepts are fundamental to working with streams and asynchronous operations in Node.js.

---

### What is the control flow function?
Control flow functions are used to dictate the order in which specific code blocks or functions are executed. These functions are used to manage the flow of execution within a program, enabling developers to handle asynchronous operations, iterate through collections, handle conditional logic, and more.

---

### What is Buffer in NodeJS?
Buffer is a temporary memory, mainly used by the stream to hold some data until consumed. Buffer is mainly used to store binary data while reading from a file or receiving packets over the network.

It represents a fixed-length sequence of bytes. It allocated memory outside of the V8 heap. (Buffer not available in browser’s JavaScript)

A simple example of converting a string into a buffer:

```
// a string
const str = "Hey. this is a string!";

// convert string to Buffer
const buff = Buffer.from(str, "utf-8");

console.log(buff); // <Buffer 48 65 79 2e ... 72 69 6e 67 21>


To convert buffer into a string:

// if the buffers contain text
buffer.toString(encoding) // encoding = 'utf-8'

// if you know how many bytes the buffer contains then
buffer.toString(encoding, 0, numberOfBytes) // numberOfBytes = 12
```

---

### What is Stream and different types of streams in nodejs?
Stream is the object (abstract interface) that allows us to transfer data from source to destination and vice-versa. It enables you to process large amounts of data chunk by chunk, without having to load the entire data set into memory at once.

There are 4 types of streams:

a)Readable Stream: A Readable stream represents a source from which data can be read. It emits “data” events whenever new data becomes available. An example of a Readable stream is reading a file line by line:

```
const fs = require('fs');
const readline = require('readline');

const readableStream = fs.createReadStream('file.txt');

const readlineInterface = readline.createInterface({
  input: readableStream,
  output: process.stdout
});

readlineInterface.on('line', (line) => {
  console.log(line);
});
```

b)Writable Stream: A Writable stream represents a destination to which data can be written. It can be used to write data to files, HTTP responses, or any other writable target. Here’s an example of writing data to a file using a Writable stream:

```
const fs = require('fs');

const writableStream = fs.createWriteStream('output.txt');

writableStream.write('Hello, ');
writableStream.write('World!');
writableStream.end();
```
c)Duplex Stream: A Duplex stream is both readable and writable, allowing both data input and output. A common example is a TCP socket, where data can be both read from and written to. Here’s a simple echo server using a Duplex stream:
```
const net = require('net');

const server = net.createServer((socket) => {
  socket.pipe(socket);
});

server.listen(3000);
```

d)Transform Stream: A Transform stream is a Duplex stream that performs transformations on the data as it is read and written. It allows for data modification or manipulation. An example of a Transform stream is compressing or encrypting data on the fly:

```
const fs = require('fs');
const zlib = require('zlib');

const readableStream = fs.createReadStream('file.txt');
const writableStream = fs.createWriteStream('file.txt.gz');
const gzipStream = zlib.createGzip();

readableStream.pipe(gzipStream).pipe(writableStream);
```
---

### What are processes and threads and How do they communicate between multiple threads and processes?
**PROCESS**: A process in Node.js refers to an instance of the Node.js runtime that can be executed independently. Each Node.js process has its own memory space, global objects, modules, and event loop.

When you run a Node.js application, you’re essentially starting a process. Node.js applications can be single-threaded, meaning they run in a single process, or they can leverage the built-in clustering module to create multiple processes to take advantage of multi-core systems.

Create Multiple Processes: The Cluster module in Node.js is designed specifically to enable efficient load balancing of incoming network connections across multiple processes. It allows a Node.js application to take advantage of multi-core systems by creating multiple instances of the application, each running in its own process.

**The primary purpose of the Cluster module is to distribute incoming connection requests** (e.g., HTTP requests) across a pool of workers, allowing a Node.js server to handle multiple requests concurrently. This is achieved by forking the main Node.js process into multiple workers using a master/worker architecture.

```
const cluster = require('cluster');
const http = require('http');
const numCPUs = require('os').cpus().length;

if (cluster.isMaster) {
  // Create a worker process for each CPU core
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }

  // Listen for when a worker exits and replace it
  cluster.on('exit', (worker, code, signal) => {
    console.log(`Worker ${worker.process.pid} died. Restarting...`);
    cluster.fork();
  });
} else {
  // Worker processes create an HTTP server
  http.createServer((req, res) => {
    res.writeHead(200);
    res.end('Hello, world!\n');
  }).listen(8000);
}
```

**THREAD**: Node.js, by default, uses a single-threaded, event-driven architecture, meaning it utilizes a single thread (event loop) to execute JavaScript code. This single thread processes I/O operations (like file system and network operations) and asynchronous events.

However, Node.js does use additional threads from a thread pool managed by the libuv library to handle certain operations, such as file system operations.

It is also possible to create multiple threads by utilizing Worker Threads, a module provided by Node.js for handling heavy CPU-bound computation and parallel processing.

```
const { Worker, isMainThread, parentPort } = require('worker_threads');

if (isMainThread) {
  // This code is executed in the main thread
  // Create new worker threads
  const worker1 = new Worker(__filename);
  const worker2 = new Worker(__filename);

  // Set up message handlers for worker threads
  worker1.on('message', (msg) => console.log('Message from worker 1:', msg));
  worker2.on('message', (msg) => console.log('Message from worker 2:', msg));

  // Post messages to worker threads
  worker1.postMessage('Hello from main thread to worker 1');
  worker2.postMessage('Hello from main thread to worker 2');
} else {
  // This code is executed in the worker threads
  // Set up message handler for worker thread
  parentPort.on('message', (msg) => {
    console.log('Message from main thread:', msg);
    // Send message back to the main thread
    parentPort.postMessage('Hello from worker thread');
  });
}
```
here the main thread creates two worker threads using the Worker class. Each worker thread listens for messages from the main thread using the on(‘message’ event listener and sends messages back to the main thread using parentPort.postMessage().

When running this code, you would see messages exchanged between the main thread and the worker threads, demonstrating the inter-thread communication.

Communication between Threads: you can pass data between threads using the Worker and parentPort objects provided by the Worker Threads module.

```
// main.js
const { Worker } = require('worker_threads');

// Create a new worker thread and pass initial data
const worker = new Worker('./worker.js', { workerData: { message: 'Hello from main thread' } });

// Listen for messages from the worker thread
worker.on('message', (data) => {
  console.log('Message from worker thread:', data);
});
```

```
// worker.js
const { workerData, parentPort } = require('worker_threads');

// Receive initial data from the main thread
console.log('Received data from main thread:', workerData);

// Send a message to the main thread
parentPort.postMessage('Hello from worker thread');
```
In this example, the main.js file creates a new worker thread using Worker(‘./worker.js’) and passes initial data to the worker using the workerData option. In the worker.js file, the workerData is received from the main thread, and a message is sent back to the main thread using parentPort.postMessage().

When the worker script is executed, calling parentPort.postMessage() will trigger the ‘message’ event in the main thread, and the data passed from the worker thread can be accessed and processed accordingly.

This approach allows threads to communicate with each other and pass data back and forth as needed, enabling efficient multi-threaded data processing in Node.js.

---

### Difference between put, post, and patch methods?
The POST method is used to submit data to a server, the PUT method is used to replace or create a resource at a specific URL, and the PATCH method is used to apply partial modifications to a resource.

---

### What is the use of module.export ?
module.exports is a special object in Node.js that is used to expose a module’s contents to be consumed by other modules. When a module is required in another module using require, the value returned is the module.exports object of the required module.

```
// myModule.js
const myFunction = () => {
  console.log('This is my function');
};

const myVariable = 'This is my variable';

module.exports.myFunction = myFunction;
module.exports.myVariable = myVariable;
```

In the above example, the module.exports object is used to expose the myFunction and myVariable to other modules. Another module can consume myModule.js using require and access myFunction and myVariable like this:

```
// anotherModule.js
const myModule = require('./myModule');

myModule.myFunction(); // Output: This is my function
console.log(myModule.myVariable); // Output: This is my variable
```

---

### What is the node process model?
The Node.js process model revolves around the event-driven, single-threaded, non-blocking architecture, which optimizes the handling of concurrent operations and I/O-bound activities, making it well-suited for building scalable, high-performance applications.

Here’s a breakdown of the key components:

Event-Driven
Single-Threaded
Non-Blocking I/O
Worker Threads (Optional)

---

### What is CORS?
CORS stands for Cross-Origin Resource Sharing. It is a security feature implemented in web browsers to restrict web pages from making requests to a different domain than the one that served the web page.

In a Node.js application, you can set up CORS handling using middleware such as the ‘cors’ package or by manually adding the necessary CORS headers to responses. This allows you to control which origins have permission to access resources in your Node.js application.

CORS is a security feature that regulates cross-origin requests, allowing secure communication between different domains while protecting users’ data from unauthorized access and potential security threats.

When you visit a website, let’s say “example.com”, your web browser allows JavaScript code running on “example.com” to make requests to the same domain. This is part of the security protocol called the same-origin policy, and it’s meant to protect users’ data from malicious attacks.

However, sometimes a web page might need to make requests to a different domain. For example, let’s say “example.com” needs to retrieve some data from “api.otherdomain.com”. This is where CORS comes into play.

### Difference between crypto and bcrypt module?
crypto Module: The crypto module in Node.js provides cryptographic functionality, including encryption, hashing, and decryption. It offers a wide range of cryptographic algorithms and tools for handling secure data transformations.

```
const crypto = require('crypto');

   const password = 'mySecurePassword';
   const salt = crypto.randomBytes(16).toString('hex'); // Generate a random salt
   const hash = crypto.pbkdf2Sync(password, salt, 100000, 64, 'sha512').toString('hex'); // Generate a hashed password

   console.log('Salt:', salt);
   console.log('Hashed Password:', hash);

```

bcrypt Module: The bcrypt module is specifically designed for password hashing using the bcrypt algorithm. It provides a convenient way to securely hash passwords, a common requirement for user authentication systems. bcrypt automatically handles the generation of salts, which enhances the security of the hashed passwords.

```
   const bcrypt = require('bcrypt');
   const saltRounds = 10;
   const myPlaintextPassword = 'mySecurePassword';

   bcrypt.hash(myPlaintextPassword, saltRounds, function(err, hash) {
     if (!err) {
       console.log('Hashed Password:', hash);
     }
   });
   ```
---

16. Difference between NPM, YARN, and NPX?
NPM is the default package manager for Node.js and is used to install, manage, and publish packages. It is bundled with Node.js installation.

Yarn is a popular alternative to NPM for package management in Node.js, with additional features and enhanced performance compared to NPM. It was developed by Facebook and offers similar features to NPM. Yarn is faster than NPM and NPX.

NPX is a tool that comes with NPM and is used to execute Node.js packages without having to install them globally. It allows you to run packages directly from the NPM registry or execute binaries from local node_modules/.bin folders

```

17. What is LTS release?
An LTS (Long Term Support) release is a specific version of the software that is designated for long-term maintenance and support.

```

18. Difference between req.params and req.query?
In the context of a Node.js server using the Express framework, req.params and req.query are used to access different types of parameters passed in the URL.

req.params is an object containing properties mapped to the named route parameters. These parameters are part of the URL path and are matched by the route’s path pattern. They are typically used to capture dynamic values from the URL.

```
// Route definition
app.get('/users/:id', (req, res) => {
    const userId = req.params.id; // Access the "id" parameter from the URL
    // Use userId to retrieve user data
});
```

req.query is an object containing a property for each query string parameter in the route. Query parameters are appended to the URL after a ? and are used to pass additional information or filters for the requested resource.
If you have a route defined as /search, and a client makes a request like /search?city=NewYork&active=true, you can access the query parameters city and active using req.query.city and req.query.active respectively.
```
  // Route definition
  app.get('/search', (req, res) => {
      const city = req.query.city; // Access the "city" query parameter
      const active = req.query.active; // Access the "active" query parameter
      // Use city and active for searching
  });
  ```
---

Difference between dependency and dev-dependency?
Dependencies are the packages that are required for the application to run in the production environment.

DevDependencies are the packages that are only needed for development and testing purposes. These packages include tools, libraries, and utilities that are used during the development, testing, and build process, but are not required for the application to function in the production environment.

---

20. What is the error first callback function?
The “error-first callback” pattern, also known as “Node.js-style callbacks”, is a convention used in Node.js for handling asynchronous operations. In this pattern, callback functions are structured to take an error as the first parameter, allowing the calling code to check for and handle errors in a consistent manner.
```
function readFileAndHandleError(path, callback) {
  fs.readFile(path, 'utf8', (err, data) => {
    if (err) {
      // Pass the error to the callback as the first parameter
      callback(err);
    } else {
      // Pass the data to the callback as the second parameter
      callback(null, data);
    }
  });
}

// Example usage of the error-first callback function
readFileAndHandleError('example.txt', (err, data) => {
  if (err) {
    console.error('An error occurred:', err);
  } else {
    console.log('File data:', data);
  }
});
```
---

21. Difference between Authentication and Authorization?
Authentication is the process of validating the identity of a user or entity, through the credentials, such as usernames, passwords, biometric data, or security tokens.

Authorization is the process of determining the rights and privileges of a user to access the resources. Authorization typically involves specifying what resources or operations a user can interact with based on their role, group membership, or other relevant attributes.

---

22. How to handle errors in node?
In Node.js, errors can be handled using a variety of techniques, including try…catch blocks, error-first callbacks, and the use of error event emitters.

```
Using try-catch:

try {
  // Code that may throw an error
  const result = someFunction();
  console.log(result);
} catch (error) {
  console.error('An error occurred:', error);
}
Using event emitters:

const myEmitter = new MyEmitter();

myEmitter.on('error', (err) => {
  console.error('An error occurred:', err);
});

myEmitter.emit('error', new Error('Oops! Something went wrong.'));
```
---

23. How to resolve unhandled exceptions in node?
In Node.js, unhandled exceptions can be resolved using the process.on(‘uncaughtException’) event. By attaching a listener to this event, you can catch unhandled exceptions and prevent Node.js from terminating.

```
process.on('uncaughtException', (err) => {
  console.error('An unhandled exception occurred:', err);
  // Perform cleanup, logging, or any necessary action
  // Avoid attempting to continue with the application as it may be in an inconsistent state
  // Gracefully shut down the application
  process.exit(1); // Exit the process with a failure code (1)
});

// Example of an unhandled exception (for demonstration purposes)
// This code will throw an unhandled exception
setTimeout(() => {
  throw new Error('Intentional unhandled exception');
}, 100);

// Other application logic
// ...

```
When an unhandled exception occurs, the provided callback function is executed, allowing you to log the error, perform any necessary cleanup, and gracefully shut down the application using process.exit(1).

---

24. What is cron job?
A cron job is a time-based job scheduler. It allows users to schedule tasks (commands or scripts) to run periodically at fixed times, dates, or intervals.

In Node.js, you can use the node-cron module to schedule jobs to run at specific times.

```
const cron = require('node-cron');

// Schedule a job to run every minute
cron.schedule('* * * * *', () => {
  console.log('Running scheduled job every minute');
});
```
The first argument ‘* * * * *’ represents the cron expression for running the job every minute. The second argument … is a function that will be executed when the scheduled time is reached.

Run a Job Every Hour:
```
   const cron = require('node-cron');

   // Schedule a job to run every hour
   cron.schedule('0 * * * *', () => {
     console.log('Running scheduled job every hour');
   });
```

Run a Job Every Day at a Specific Time:
```
   const cron = require('node-cron');

   // Schedule a job to run at 8:00 AM every day
   cron.schedule('0 8 * * *', () => {
     console.log('Running scheduled job at 8:00 AM every day');
   });
```
---

25. What difference between fork and spawn and exec methods?
The fork method is specifically designed for creating child processes that run Node.js modules. It is commonly used to create new instances of the Node.js interpreter to run separate Node.js scripts.
The spawn method is a more general-purpose function for creating child processes. It is used to spawn new processes and execute commands in the operating system’s shell. With spawn, you can execute non-Node.js, programs, such as Python scripts or shell commands.
Child process is created using fork, it automatically sets up a communication channel for inter-process communication (IPC) with the parent process.
With spawn, if you need to establish communication between the parent and child processes, you have to manually set up the standard input and output streams to exchange data and messages.
The exec method runs a command in a shell and buffers the output. It is suitable for simple commands and scripts where the output is not excessively large.
Example of Spawn method

```
const { spawn } = require('child_process');

const pythonProcess = spawn('python', ['script.py', 'arg1', 'arg2']);

pythonProcess.stdout.on('data', (data) => {
  console.log(`Python script stdout: ${data}`);
});

pythonProcess.stderr.on('data', (data) => {
  console.error(`Python script stderr: ${data}`);
});

pythonProcess.on('close', (code) => {
  console.log(`Python script child process exited with code ${code}`);
});
```

Example of Fork method
```
const { fork } = require('child_process');

const child = fork('child.js');

child.on('message', (message) => {
  console.log('Received message from child:', message);
});

child.send('Hello from parent!');
```

Example of Exec method
```
const { exec } = require('child_process');

// Execute a simple shell command to list files in the current directory
exec('ls -l -a', (error, stdout, stderr) => {
  if (error) {
    console.error(`error: ${error.message}`);
    return;
  }
  if (stderr) {
    console.error(`stderr: ${stderr}`);
    return;
  }
  console.log(`stdout: ${stdout}`);
});
```
---

26. What are the global objects of Node.js?
Node.js Global Objects are the objects that are available in all modules. Global Objects are built-in objects that are part of JavaScript and can be used directly in the application without importing any particular module.

global: The global object is the global namespace object in Node.js. It acts as a container for global variables and functions. Any variable or function defined on the global object becomes available across modules.
process: The process object provides information about the current Node.js process and allows you to control the process. It contains properties such as process.env, process.argv, and methods like process.exit().
console: The console object provides a simple debugging console that is similar to the console mechanism provided by web browsers. It includes functions like console.log(), console.error(), and console.warn().
Buffer: The Buffer class provides a way to work with binary data directly. Buffers are used in Node.js to handle raw binary data for tasks such as reading from or writing to the file system, dealing with network operations, or handling binary data in other formats.
__filename: The __filename variable represents the name of the current file.
__dirname: The __dirname variable represents the directory name of the current module.
setTimeout and setInterval: Node.js provides global functions setTimeout and setInterval for scheduling code execution after a specified delay or at regular intervals, similar to their behavior in browsers.

---

